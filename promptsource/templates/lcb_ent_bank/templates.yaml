dataset: lcb_ent_bank
templates:
  af1fa9cd-a73a-40f2-8fc9-0d2e5e97dcad: !Template
    answer_choices: incorrect ||| correct
    id: af1fa9cd-a73a-40f2-8fc9-0d2e5e97dcad
    jinja: "
{% for p,t in zip(premises, truths) %}\
The statement \"{{ p }}\" is {{ answer_choices[t] }}.\n\
{% endfor %}\
\n\
Answering \"{{ question }}\" with \"{{ answer }}\" is ||| {{ answer_choices[label] }}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
        - en
      metrics:
        - Accuracy
      original_task: true
    name: truth
    reference: ''

  39f34a10-fe49-4ca8-a998-9457d5781f05: !Template
    answer_choices: incorrect ||| correct
    id: 39f34a10-fe49-4ca8-a998-9457d5781f05
    jinja: "
Question: \"{{ question }}\" \n\n\
{% for p,t in zip(premises, truths) %}\
The statement \"{{ p }}\" is {{ answer_choices[t] }}.\n\
{% endfor %}\
\n\
Answering the question with \"{{ answer }}\" is ||| ({{ answer_choices[label] }})"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
        - en
      metrics:
        - Accuracy
      original_task: true
    name: truth-question
    reference: ''